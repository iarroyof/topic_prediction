{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iarroyof/topic_prediction/blob/master/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMxkX2ouFiA8"
      },
      "outputs": [],
      "source": [
        "!pip install stable-baselines3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "UqQzWbpWDjqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.core import ActionWrapper\n",
        "from scipy.sparse.bsr import bsr_matrix\n",
        "from numpy import random\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "import operator\n",
        "import gym\n",
        "from gym import spaces\n",
        "import statistics\n",
        "import os\n",
        "from random import *\n",
        "import scipy.sparse as sparse\n",
        "#from stable_baselines3.common#.policies# import MlpPolicy\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "\n",
        "#from stable_baselines3 import A2C\n",
        "#from stable_baselines.a2c import A2C\n",
        "\n",
        "class Environment(gym.Env):\n",
        "\n",
        "   metadata = {'render.modes': ['human']}\n",
        "\n",
        "   def __init__(self, objeto, thd):\n",
        "    \n",
        "    self.busc = objeto\n",
        "    self.info ={}\n",
        "    self.rwd_th= thd\n",
        "    self.step_counter = 0\n",
        "    self.vocab_tam = self.busc.vocab_tam\n",
        "    self.vocab = self.busc.vocab_list\n",
        "    self.action_space=spaces.MultiDiscrete((self.vocab_tam,self.vocab_tam, self.vocab_tam, self.vocab_tam)) #numpy.ndarray\n",
        "    self.observation_space=gym.spaces.Box(low=0.0,high=10.0,shape=(self.vocab_tam,),dtype=np.float32) #numpy.ndarray\n",
        "    print(\"space observation shape:\",self.observation_space.shape)\n",
        "  \n",
        "   def set_complexity(self, complejidad):\n",
        "      self.complej_usr = complejidad\n",
        "\n",
        "   def set_topics(self, temas):\n",
        "     self.temas = temas\n",
        "\n",
        "   def step(self,action):  #action =numpy.ndarray\n",
        "     if isinstance(action, tuple):\n",
        "        action = action[0]\n",
        "     action_list = action.tolist()\n",
        "     list_vocab= [self.vocab[index] for index in action]\n",
        "     self.temas = ' '.join(str(e) for e in list_vocab)  #class 'str'\n",
        "     print(\"self.temas:\",self.temas)\n",
        "     \n",
        "     self.busc.buscar(self.temas)\n",
        "        \n",
        "     complej_indices=self.busc.complex_index\n",
        "     complej_mediana = np.median(complej_indices)\n",
        "\n",
        "     indice_simi2= self.busc.indices_sim\n",
        "     simi_c = indice_simi2.data\n",
        "     simi_median=np.median(simi_c)\n",
        "    \n",
        "     diff =abs(self.complej_usr-complej_mediana)\n",
        "     anti_complej= 1- diff\n",
        "     comp=(anti_complej+simi_median)/2\n",
        "   \n",
        "     if comp > self.rwd_th:\n",
        "       rwd=1\n",
        "     else:\n",
        "       rwd=0\n",
        "\n",
        "     self.busc.comps_complex()\n",
        "     self.busc.get_vectors()\n",
        "     obs= self.busc.vectores  #numpy.ndarray\n",
        "     r=np.min(obs)\n",
        "     #print(\"obs shape:\",obs.shape)\n",
        "     #print(\"obs:\",obs)\n",
        "     \n",
        " \n",
        "\n",
        "     self.step_counter += 1\n",
        "\n",
        "     done= False\n",
        "     info = self.info\n",
        "\n",
        "     return rwd, done, obs, info\n",
        "\n",
        "   def reset(self):\n",
        "     rwd, done, obs, info = self.step(self.action_space.sample())\n",
        "     return obs\n",
        "   \n",
        "class Buscador:\n",
        " \n",
        "        def __init__(self,db,complej_tipo):\n",
        "\n",
        "           self.conversations = db[\"CONVERSATION\"]\n",
        "           self.complejidades = db[\"Standardize\"]  \n",
        "           self.vectorizer = TfidfVectorizer()\n",
        "           self.db =db\n",
        "           self.complej_tipo=complej_tipo\n",
        "           self.indice = {}\n",
        "           self.indices_sim = {}\n",
        "           self.complex_index=list() \n",
        "           self.vectores ={} \n",
        "           \n",
        "        def fit(self):\n",
        "          self.X = self.vectorizer.fit_transform(self.conversations)\n",
        "          vocab = self.vectorizer.vocabulary_.keys()\n",
        "          self.vocab_tam=len(vocab)\n",
        "          self.vocab_list = [*vocab]\n",
        "          self.vocab_arr =np.asarray(self.vocab_list)\n",
        "          print(\"vocab_list:\",self.vocab_list)\n",
        "          #for k in vocab:\n",
        "          #     vocab_list=[k]\n",
        "          #for k in vocab:\n",
        "          #  print(k)\n",
        "          self.vocab_lista= [[k] for k in vocab]\n",
        "          #list2= [self.vocab[index] for index in action]\n",
        "       \n",
        "            \n",
        "        def buscar(self, topic):\n",
        "          q=[topic]\n",
        "          q_vec = self.vectorizer.transform(q)\n",
        "          self.indices_sim=cosine_similarity(self.X, q_vec,dense_output=False) \n",
        "          indi=self.indices_sim.todense()\n",
        "          vect=np.where(indi)\n",
        "          for i in range(len(vect)): #Get index\n",
        "            if i==0: \n",
        "             for j in range(len(vect[i])):\n",
        "               self.indice[j]=vect[i][j]\n",
        "                  \n",
        "          self.comps_complex()\n",
        "          return self.indice\n",
        "     \n",
        "        def get_vectors(self): \n",
        "          index_list=[]\n",
        "          index_list= list(self.indice.values())\n",
        "          State_index=self.X[index_list,:]\n",
        "          vectors=State_index.mean(axis=0)\n",
        "          self.vectores=np.squeeze(np.asarray(vectors))\n",
        "        \n",
        "        def comps_complex(self): \n",
        "          for i in range(len(self.indice)):#complexity index conversation\n",
        "            self.complex_index.append(self.complejidades[self.indice[i]])\n",
        "\n",
        "# nueva busqueda con temas y complejidad de usuario\n",
        "def new_search(env, topics, complexity):\n",
        "    env.set_topics(topics)\n",
        "    #print(\"Tipo del tema:\", type(topics)) #class 'str'\n",
        "    env.set_complexity(complexity)\n",
        "    obs = env.reset()\n",
        "    return obs, env\n",
        "\n",
        "def agent_model(env):\n",
        "    model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "    model.learn(total_timesteps=100)\n",
        "    #model.save(\"Bucador_Pers\")\n",
        "    return model\n",
        "\n",
        "\n",
        "db = pd.read_excel(\"RECORDING SCRIPT FCE_D.xlsx\", sheet_name=\"Hoja1\")\n",
        "b=Buscador(db,\"tipo de complejidad de vocabulario\")\n",
        "b.fit()\n"
      ],
      "metadata": {
        "id": "hKj75vVnEnrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "e=Environment(b, 0.5)\n",
        "tema=['engine','car','white' ]#, 'food','school','beer','digital','phone','music','dance']\n",
        "n_steps = 1000\n",
        "\n",
        "#usr_topics, usr_complexity = desde_interfaz()\n",
        "usr_topics, usr_complexity = \" \".join(tema), 0.2\n",
        "obs,env = new_search(e, usr_topics, usr_complexity)\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Parallel environments\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "model= PPO(\"MlpPolicy\", env)\n",
        "model.learn(n_steps)\n",
        "#model= A2C(\"MlpPolicy\", env)\n",
        "\n",
        "for _ in range(n_steps): ## episodio\n",
        "\n",
        "    action = model.predict(obs) # consulta reformulada\n",
        "    #action_arr = action[0]\n",
        "    print(action)\n",
        "    rwd, done,obs,info = env.step(action)#_arr) #obs\n",
        "\n",
        "    print(\"Agent Result:\",rwd, done, obs, info, action)\n",
        "    if done:\n",
        "        break\n"
      ],
      "metadata": {
        "id": "-BfXtKSFR1FQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}